#ML
##绪论
###定义、术语
学习/训练：从数据中学得模型的过程
训练数据、训练样本
训练集：训练样本集合
假设/真实：模型对应的潜在规律
学习器：模型，算法在数据和参数空间的实例化
$(x_{i},y_{i})$表示第i个样例，$y_{i}\in \mathcal{Y}$为示例$x_{i}$的标记，$\mathcal{Y}$为所有标记的集合，标记空间/输出空间
分类：预测离散数值
二分类：只涉及两个类别，其中一个为正类，另一个为反类
回归：预测连续值
>预测任务一般方法：通过对训练集$\{(x_{i},y_{i})|i\in[1,m]\}$进行学习，建立一个对输入空间x到输出空间y的映射$f:\mathcal{X}\mapsto\mathcal{Y}$。
>PS：对二分类任务，通常令$\mathcal{Y}=\{-1,+1\}$或$\{0,1\}$；对多分类任务，$|\mathcal{Y}|>2$；对回归任务，$\mathcal{Y}=\mathbb{R}$

测试：使用模型进行预测；测试样本
聚类：训练集元素分成若干组，每组成为一个“簇”
监督学习/无监督学习：区别为训练数据是否拥有标记信息；分类和回归时前者的代表，聚类是后者的代表
###假设空间
从样例中学习为归纳学习。
学习过程可看作一个在所有假设组成的空间中进行搜索的过程，搜索目标是找到与训练集匹配的假设。假设的表示一旦确定，假设空间及其规模大小确定。
归纳偏好：学习算法在学习过程中对某种类型假设的偏好
奥卡姆剃刀原理：若有多个假设与观察一致，则选最简单的那个（并不唯一成立）

令$P(h|X,\zeta_{a})$代表算法$\zeta_{a}$训练数据$X$产生假设$h$的概率，再令$f$代表希望学习的真是目标函数。则$\zeta_{a}$的训练集外误差，即$\zeta_{a}$在训练集外所有样本上的误差为：
$$E_{ote}(\zeta_{a}|X,f)=\sum\limits_{h}\sum\limits_{x\in\mathcal{X}-X}P(x)\mathbb{I}(h(x)\ne f(x))P(h|X,\zeta_{a})$$
其中$\mathbb{I}(\cdot)$为指示函数，即若$\cdot$为真时取值为1，否则取值为0.
以二分类问题为例，真是目标函数为任意函数$\mathcal{X}\mapsto\{0,1\}$，函数空间为$\{0,1\}^{|\mathcal{X}|}$，经过计算，对所有可能的$f$按均匀分布对误差求和：
$$\sum\limits_{f}E_{ote}(\zeta_{a}|X,f)=2^{|\mathcal{X}|-1}\sum\limits_{x\in\mathcal{X}-X}P(x)\cdot1$$
即有：
>免费午餐定理（NFL定理）：所有可能的$f$的总误差与学习算法无关，误差期望总是相同.

然而事实上，$f$并不均匀分布，因此学习算法需要考虑具体问题。